{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq langchain-community pypdf"
      ],
      "metadata": {
        "id": "K6pcyFcaPqZ6"
      },
      "id": "K6pcyFcaPqZ6",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bf37a837-7a6a-447b-8779-38f26c585887",
      "metadata": {
        "id": "bf37a837-7a6a-447b-8779-38f26c585887"
      },
      "source": [
        "# Build a semantic search engine\n",
        "\n",
        "This Project will familiarize you with LangChain's [document loader](/docs/concepts/document_loaders), [embedding](/docs/concepts/embedding_models), and [vector store](/docs/concepts/vectorstores) abstractions. These abstractions are designed to support retrieval of data--  from (vector) databases and other sources--  for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/docs/concepts/rag) (see our RAG tutorial [here](/docs/tutorials/rag)).\n",
        "\n",
        "Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query.\n",
        "\n",
        "## Concepts\n",
        "\n",
        "This guide focuses on retrieval of text data. We will cover the following concepts:\n",
        "\n",
        "- Documents and document loaders;\n",
        "- Text splitters;\n",
        "- Embeddings;\n",
        "- Vector stores and retrievers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a2ac32c4-1036-42d8-8a3d-f7f57e3a0df7",
      "metadata": {
        "id": "a2ac32c4-1036-42d8-8a3d-f7f57e3a0df7",
        "outputId": "bf434c97-f15b-42bc-c146-d755facfdeeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "file_path = \"/content/attention_is_all_you_need.pdf\"\n",
        "loader = PyPDFLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90f4800-bb82-416b-beba-f42ae88a5c66",
      "metadata": {
        "id": "b90f4800-bb82-416b-beba-f42ae88a5c66"
      },
      "source": [
        "\n",
        "\n",
        "`PyPDFLoader` loads one `Document` object per PDF page. For each, we can easily access:\n",
        "\n",
        "- The string content of the page;\n",
        "- Metadata containing the file name and page number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "850e2ca5-6b20-4e58-ad99-b19786358a3e",
      "metadata": {
        "id": "850e2ca5-6b20-4e58-ad99-b19786358a3e",
        "outputId": "a8418406-f127-4269-bc3c-997ed19a9afb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Provided proper attribution is provided, Google hereby grants permission to\n",
            "reproduce the tables and figures in this paper solely for use in journalistic or\n",
            "scholarly works.\n",
            "Attention Is All You Need\n",
            "\n",
            "\n",
            "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/attention_is_all_you_need.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "print(f\"{docs[0].page_content[:200]}\\n\")\n",
        "print(docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca6980f-4870-490a-9fe6-8caeead3c1d1",
      "metadata": {
        "id": "2ca6980f-4870-490a-9fe6-8caeead3c1d1"
      },
      "source": [
        "### Splitting\n",
        "\n",
        "For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve `Document` objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text.\n",
        "\n",
        "We can use [text splitters](/docs/concepts/text_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters\n",
        "with 200 characters of overlap between chunks. The overlap helps\n",
        "mitigate the possibility of separating a statement from important\n",
        "context related to it. We use the\n",
        "[RecursiveCharacterTextSplitter](/docs/how_to/recursive_text_splitter),\n",
        "which will recursively split the document using common separators like\n",
        "new lines until each chunk is the appropriate size. This is the\n",
        "recommended text splitter for generic text use cases.\n",
        "\n",
        "We set `add_start_index=True` so that the character index where each\n",
        "split Document starts within the initial Document is preserved as\n",
        "metadata attribute “start_index”.\n",
        "\n",
        "See [this guide](/docs/how_to/document_loader_pdf/) for more detail about working with PDFs, including how to extract text from specific sections and images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "11c16e79-c8af-4949-9363-9a93a911a0e1",
      "metadata": {
        "id": "11c16e79-c8af-4949-9363-9a93a911a0e1",
        "outputId": "2b14a5f2-446f-4be5-ea00-33b63a6c29de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "len(all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c066d46-9187-4d5a-98d3-974c37610276",
      "metadata": {
        "id": "5c066d46-9187-4d5a-98d3-974c37610276"
      },
      "source": [
        "## Embeddings\n",
        "\n",
        "Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/docs/concepts/embedding_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text.\n",
        "\n",
        "LangChain supports embeddings from [dozens of providers](/docs/integrations/text_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:\n",
        "\n",
        "import EmbeddingTabs from \"@theme/EmbeddingTabs\";\n",
        "\n",
        "<EmbeddingTabs customVarName=\"embeddings\" />"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq langchain-openai openai"
      ],
      "metadata": {
        "id": "-5q8fMw7RrVQ"
      },
      "id": "-5q8fMw7RrVQ",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "3sS89jj9SGEx"
      },
      "id": "3sS89jj9SGEx",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4d238a38-b9b3-494a-9cea-8694a1b03bc7",
      "metadata": {
        "id": "4d238a38-b9b3-494a-9cea-8694a1b03bc7"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c5f3b0ac-4e18-4c6b-84e7-e8822c59ce17",
      "metadata": {
        "id": "c5f3b0ac-4e18-4c6b-84e7-e8822c59ce17",
        "outputId": "428432df-fadb-4c25-fce0-114130087d2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated vectors of length 1536\n",
            "\n",
            "[-0.034789446741342545, 0.013344650156795979, 0.03275754675269127, -0.010440953075885773, 0.028336787596344948, 0.010571379214525223, -0.008360999636352062, -0.019920870661735535, -0.008491425774991512, -0.03121989034116268]\n"
          ]
        }
      ],
      "source": [
        "vector_1 = embeddings.embed_query(all_splits[0].page_content)\n",
        "vector_2 = embeddings.embed_query(all_splits[1].page_content)\n",
        "\n",
        "assert len(vector_1) == len(vector_2)\n",
        "print(f\"Generated vectors of length {len(vector_1)}\\n\")\n",
        "print(vector_1[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cac19bd-27d1-40f1-9c27-7a586b685b4e",
      "metadata": {
        "id": "1cac19bd-27d1-40f1-9c27-7a586b685b4e"
      },
      "source": [
        "Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search.\n",
        "\n",
        "## Vector stores\n",
        "\n",
        "LangChain [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html) objects contain methods for adding text and `Document` objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/docs/how_to/embed_text) models, which determine how text data is translated to numeric vectors.\n",
        "\n",
        "LangChain includes a suite of [integrations](/docs/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/docs/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:\n",
        "\n",
        "import VectorStoreTabs from \"@theme/VectorStoreTabs\";\n",
        "\n",
        "<VectorStoreTabs/>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq langchain-chroma"
      ],
      "metadata": {
        "id": "Df6k7GVaSk4T"
      },
      "id": "Df6k7GVaSk4T",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5b0d3730-c008-4246-8b03-dd3058513e1c",
      "metadata": {
        "id": "5b0d3730-c008-4246-8b03-dd3058513e1c"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "vector_store = Chroma(embedding_function=embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b3035f-1371-4965-ab7a-04eae25e47f3",
      "metadata": {
        "id": "e3b3035f-1371-4965-ab7a-04eae25e47f3"
      },
      "source": [
        "Having instantiated our vector store, we can now index the documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2ea92e04-331c-4f83-aa2a-508322bdfbfc",
      "metadata": {
        "id": "2ea92e04-331c-4f83-aa2a-508322bdfbfc"
      },
      "outputs": [],
      "source": [
        "ids = vector_store.add_documents(documents=all_splits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff0f0b43-e5b8-4c79-b782-a02f17345487",
      "metadata": {
        "id": "ff0f0b43-e5b8-4c79-b782-a02f17345487"
      },
      "source": [
        "Note that most vector store implementations will allow you to connect to an existing vector store--  e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/docs/integrations/vectorstores) for more detail.\n",
        "\n",
        "Once we've instantiated a `VectorStore` that contains documents, we can query it. [VectorStore](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html) includes methods for querying:\n",
        "- Synchronously and asynchronously;\n",
        "- By string query and by vector;\n",
        "- With and without returning similarity scores;\n",
        "- By similarity and [maximum marginal relevance](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStore.html#langchain_core.vectorstores.base.VectorStore.max_marginal_relevance_search) (to balance similarity with query to diversity in retrieved results).\n",
        "\n",
        "The methods will generally include a list of [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) objects in their outputs.\n",
        "\n",
        "### Usage\n",
        "\n",
        "Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document.\n",
        "\n",
        "Return documents based on similarity to a string query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "7e01ed91-1a98-4221-960a-bd7a2541a548",
      "metadata": {
        "id": "7e01ed91-1a98-4221-960a-bd7a2541a548",
        "outputId": "bee5a3d5-ff84-4ca7-a09c-ad71da83e661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='3.2 Attention\n",
            "An attention function can be described as mapping a query and a set of key-value pairs to an output,\n",
            "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n",
            "3' metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 2, 'page_label': '3', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 1610, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}\n"
          ]
        }
      ],
      "source": [
        "results = vector_store.similarity_search(\n",
        "    \"What is attention Mechanism?\"\n",
        ")\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d4f9857-5a7d-4b5f-82b8-ff76539143c2",
      "metadata": {
        "id": "4d4f9857-5a7d-4b5f-82b8-ff76539143c2"
      },
      "source": [
        "Async query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7ff9e061-7710-40b2-93dc-1ca2b71ef96d",
      "metadata": {
        "id": "7ff9e061-7710-40b2-93dc-1ca2b71ef96d",
        "outputId": "e8c9b674-1884-4d91-d448-000c1210865b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\n",
            "where headi = Attention(QWQ\n",
            "i , KWK\n",
            "i , V WV\n",
            "i )\n",
            "Where the projections are parameter matricesWQ\n",
            "i ∈ Rdmodel×dk , WK\n",
            "i ∈ Rdmodel×dk , WV\n",
            "i ∈ Rdmodel×dv\n",
            "and WO ∈ Rhdv×dmodel .\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,' metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}\n"
          ]
        }
      ],
      "source": [
        "results = await vector_store.asimilarity_search(\"What is Multi Head Attention?\")\n",
        "\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4172698-9ad7-4422-99b2-bdc268e99c75",
      "metadata": {
        "id": "d4172698-9ad7-4422-99b2-bdc268e99c75"
      },
      "source": [
        "Return scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "52dfc576-40a7-4030-aeb5-bb4d3a493e3e",
      "metadata": {
        "id": "52dfc576-40a7-4030-aeb5-bb4d3a493e3e",
        "outputId": "0aaa2eed-4808-43b8-cef0-7552fe5081b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.2624851167201996\n",
            "\n",
            "page_content='output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\n",
            "where headi = Attention(QWQ\n",
            "i , KWK\n",
            "i , V WV\n",
            "i )\n",
            "Where the projections are parameter matricesWQ\n",
            "i ∈ Rdmodel×dk , WK\n",
            "i ∈ Rdmodel×dk , WV\n",
            "i ∈ Rdmodel×dv\n",
            "and WO ∈ Rhdv×dmodel .\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,' metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}\n"
          ]
        }
      ],
      "source": [
        "# Note that providers implement different scores; the score here\n",
        "# is a distance metric that varies inversely with similarity.\n",
        "\n",
        "results = vector_store.similarity_search_with_score(\"What is Multi Head Attention?\")\n",
        "doc, score = results[0]\n",
        "print(f\"Score: {score}\\n\")\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4991642-7275-40a9-b11a-e3beccbf2614",
      "metadata": {
        "id": "b4991642-7275-40a9-b11a-e3beccbf2614"
      },
      "source": [
        "Return documents based on similarity to an embedded query:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7be726c1-b24c-414a-9862-d412b94784b2",
      "metadata": {
        "id": "7be726c1-b24c-414a-9862-d412b94784b2",
        "outputId": "04a5c8be-5e88-4bde-9bcd-2109a8efdeaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='output values. These are concatenated and once again projected, resulting in the final values, as\n",
            "depicted in Figure 2.\n",
            "Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "MultiHead(Q, K, V) = Concat(head1, ...,headh)WO\n",
            "where headi = Attention(QWQ\n",
            "i , KWK\n",
            "i , V WV\n",
            "i )\n",
            "Where the projections are parameter matricesWQ\n",
            "i ∈ Rdmodel×dk , WK\n",
            "i ∈ Rdmodel×dk , WV\n",
            "i ∈ Rdmodel×dv\n",
            "and WO ∈ Rhdv×dmodel .\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "3.2.3 Applications of Attention in our Model\n",
            "The Transformer uses multi-head attention in three different ways:\n",
            "• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,' metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}\n"
          ]
        }
      ],
      "source": [
        "embedding = embeddings.embed_query(\"What is Multi Head Attention?\")\n",
        "\n",
        "results = vector_store.similarity_search_by_vector(embedding)\n",
        "print(results[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "168dbbec-ea97-4cc9-bb1a-75519c2d08af",
      "metadata": {
        "id": "168dbbec-ea97-4cc9-bb1a-75519c2d08af"
      },
      "source": [
        "\n",
        "\n",
        "## Retrievers\n",
        "\n",
        "LangChain `VectorStore` objects do not subclass [Runnable](https://python.langchain.com/api_reference/core/index.html#langchain-core-runnables). LangChain [Retrievers](https://python.langchain.com/api_reference/core/index.html#langchain-core-retrievers) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs).\n",
        "\n",
        "We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity_search` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "58b8e826-1556-489c-b27b-a1efbc4cd689",
      "metadata": {
        "id": "58b8e826-1556-489c-b27b-a1efbc4cd689",
        "outputId": "2e9fe4ed-abcb-47e6-d4c8-18505ea70a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='8eea3115-aa7f-46db-ba4d-9cc457156d22', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 2341, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-')],\n",
              " [Document(id='992b944c-87fd-4ac9-a223-4529dedd653f', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,')]]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "from typing import List\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import chain\n",
        "\n",
        "\n",
        "@chain\n",
        "def retriever(query: str) -> List[Document]:\n",
        "    return vector_store.similarity_search(query, k=1)\n",
        "\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"What is Self Attention?\",\n",
        "        \"What is Multi Head Attention?\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36d3f64-a8bc-4baa-b2ea-07e324a0143e",
      "metadata": {
        "id": "a36d3f64-a8bc-4baa-b2ea-07e324a0143e"
      },
      "source": [
        "Vectorstores implement an `as_retriever` method that will generate a Retriever, specifically a [VectorStoreRetriever](https://python.langchain.com/api_reference/core/vectorstores/langchain_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search_type` and `search_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4989fe5e-ac58-4751-bc35-f53ff885860c",
      "metadata": {
        "id": "4989fe5e-ac58-4751-bc35-f53ff885860c",
        "outputId": "9a18197d-3a9b-414e-b073-f7bbb388ea98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Document(id='8eea3115-aa7f-46db-ba4d-9cc457156d22', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 2341, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-')],\n",
              " [Document(id='992b944c-87fd-4ac9-a223-4529dedd653f', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/attention_is_all_you_need.pdf', 'start_index': 0, 'subject': '', 'title': '', 'total_pages': 15, 'trapped': '/False'}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,')]]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "retriever = vector_store.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "\n",
        "retriever.batch(\n",
        "    [\n",
        "        \"What is Self Attention?\",\n",
        "        \"What is Multi Head Attention?\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b79ded3-39ed-4aeb-8b70-cd36795ae239",
      "metadata": {
        "id": "6b79ded3-39ed-4aeb-8b70-cd36795ae239"
      },
      "source": [
        "`VectorStoreRetriever` supports search types of `\"similarity\"` (default), `\"mmr\"` (maximum marginal relevance, described above), and `\"similarity_score_threshold\"`. We can use the latter to threshold documents output by the retriever by similarity score."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xqBDZltsUrZg"
      },
      "id": "xqBDZltsUrZg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}